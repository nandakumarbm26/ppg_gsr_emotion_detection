{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.signal import find_peaks\n",
    "import matplotlib.pyplot as plt\n",
    "df=pd.read_csv('./dataset_transoform/PPG/ppg_01.csv',header=None)\n",
    "\n",
    "def nn_intervals_from_signal(signal,sampling_freq:int):\n",
    "    dt = 1/sampling_freq # Sampling interval\n",
    "    peaks, _ = find_peaks(signal, height=0.5*np.max(signal),distance=100)\n",
    "    peaks_diff=peaks*dt\n",
    "    nn_intervals = np.diff(peaks_diff)\n",
    "    return nn_intervals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import signal \n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import skew,kurtosis,entropy,median_abs_deviation\n",
    "\n",
    "def onecycle(ppg):\n",
    "        b, a = signal.butter(2, 0.5, 'highpass')\n",
    "        ppg1 = signal.filtfilt(b, a, ppg)\n",
    "\n",
    "        # Normalize the data to have a mean of 0 and standard deviation of 1\n",
    "        ppg1 = (ppg1 - np.mean(ppg1)) / np.std(ppg1)\n",
    "\n",
    "        rpeaks, = ecg.hamilton_segmenter(signal=ppg1, sampling_rate=128)\n",
    "        rr_intervals = np.diff(rpeaks)\n",
    "\n",
    "        # Compute the time-domain HRV features\n",
    "        hrv_mean = np.mean(rr_intervals)\n",
    "        hrv_sdnn = np.std(rr_intervals)\n",
    "        hrv_rmssd = np.sqrt(np.mean(np.square(np.diff(rr_intervals))))\n",
    "\n",
    "        length=len(ppg)\n",
    "        peaks, _ = find_peaks(ppg, distance=15)\n",
    "        min1=9999999\n",
    "        min2=9999999\n",
    "        for i in range(peaks[0],peaks[1]):\n",
    "            if ppg[i]<min1:\n",
    "                min1=i\n",
    "        for i in range(peaks[1],peaks[2]):\n",
    "            if ppg[i]<min2:\n",
    "                min2=i\n",
    "        if abs(peaks[2]-peaks[1])>22.5:\n",
    "            min2=peaks[0]+min1\n",
    "    #     print(min1,min2)\n",
    "        final=ppg[min1-1:min2]\n",
    "        final=ppg\n",
    "        mean=np.mean(final)\n",
    "        std=np.std(final)\n",
    "        median = np.median(final)\n",
    "        skewness=skew(final)\n",
    "        kurt=kurtosis(final)\n",
    "        p10,p25,p30,p50,p75,p80,p90 = np.nanpercentile(final,[10,25,30,50,75,80,90])\n",
    "        IQR = p75-p25\n",
    "        EN = entropy(final)\n",
    "        mid_AD = median_abs_deviation(final)\n",
    "        mean_AD = sum([abs(i-np.mean(final)) for i in final])/len(final)\n",
    "        RMS =  np.sqrt(sum([i**2 for i in final])/len(final))\n",
    "        spec_EN = entropy(final)/np.log2(len(final))\n",
    "\n",
    "        ppg_diff = np.diff(ppg)\n",
    "        # calculate the second derivative of the PPG signal\n",
    "        ppg_diff2 = np.diff(ppg_diff)\n",
    "        # calculate the activity, mobility, and complexity of the PPG signal\n",
    "        activity = np.var(ppg)\n",
    "        mobility = np.sqrt(np.var(ppg_diff) / activity)\n",
    "        complexity = np.sqrt(np.var(ppg_diff2) / np.var(ppg_diff))\n",
    "\n",
    "        env = np.abs(signal.hilbert(ppg))\n",
    "        # calculate the mean and standard deviation of the envelope\n",
    "        env_mean = np.mean(env)\n",
    "        env_std = np.std(env)\n",
    "        # calculate the mean and standard deviation of the amplitude modulation\n",
    "        am = ppg / env\n",
    "        am_mean = np.mean(am)\n",
    "        am_std = np.std(am)\n",
    "        #print([mean,std,skewness,kurt,p10,p25,p30,p50,p75,p80,p90])\n",
    "        return [hrv_rmssd,hrv_sdnn, hrv_mean,mean,median,std,kurt,skewness,p10,p25,p30,p50,p75,p80,p90,IQR,EN,mid_AD,mean_AD,RMS,spec_EN,activity,mobility,complexity,env_mean,env_std,am_mean,am_std]\n",
    "\n",
    "def gsr_features(input_signal):\n",
    "    # Compute skin conductance level (SCL)\n",
    "    scl = np.mean(input_signal)\n",
    "    # Compute skin conductance response (SCR)\n",
    "    scr = np.max(input_signal) - np.min(input_signal)\n",
    "    # Compute rise time and half-recovery time of SCR\n",
    "    peaks, _ = signal.find_peaks(input_signal, prominence=0.1)\n",
    "    rise_time = peaks[0] - np.argmin(input_signal[:peaks[0]])\n",
    "    half_recovery_time = peaks[0] - np.argmin(input_signal[peaks[0]:]) + np.argmax(input_signal[peaks[0]:])\n",
    "    # Compute peak amplitude and area under the curve (AUC) of SCR\n",
    "    peak_amp = np.max(input_signal) - np.min(input_signal[:peaks[0]])\n",
    "    auc = np.trapz(input_signal[:peaks[0]])\n",
    "    # Compute number of SCRs\n",
    "    n_scr = len(peaks)\n",
    "    # Compute slope and intercept of tonic GSR\n",
    "    x = np.arange(len(input_signal))\n",
    "    slope, intercept = np.polyfit(x, input_signal, 1)\n",
    "    # Compute latency and duration of the GSR response\n",
    "    if n_scr > 0:\n",
    "        latencies = peaks - np.argmin(input_signal[:peaks[0]])\n",
    "        durations = np.diff(peaks)\n",
    "        mean_latency = np.mean(latencies)\n",
    "        mean_duration = np.mean(durations)\n",
    "    else:\n",
    "        mean_latency = 0\n",
    "        mean_duration = 0\n",
    "\n",
    "    return [scl,scr,rise_time,half_recovery_time,peak_amp,auc,n_scr,slope,intercept,mean_latency,mean_duration]\n",
    "\n",
    "\n",
    "def nn_intervals_time_domain_features(input_signal):\n",
    "    meanNN = np.mean(input_signal)\n",
    "    medianNN = np.median(input_signal)\n",
    "    sdnn = np.std(input_signal)\n",
    "    nn50 = [x>0.05 for x in np.diff(input_signal)].count(True)\n",
    "    pNN50 = nn50 / len(input_signal) * 100\n",
    "    rmssd = np.sqrt(np.mean(np.square(np.diff(input_signal))))\n",
    "    return [meanNN,medianNN,nn50,pNN50,rmssd,sdnn]\n",
    "\n",
    "def freq_domain_features_from_signal(input):\n",
    "    \"\"\" \n",
    "    params : input_signal -> np array\n",
    "    params: fs -> int\n",
    "\n",
    "    VLFa area of VLF (0-0.04hz)\n",
    "    LFa area of VLF   (0.0.4-0.15hz)\n",
    "    HFa area of VLF   (0.15-0.4hz)\n",
    "    TOTALa total area of all band\n",
    "    VLFh highest power in VLF\n",
    "    LFh highest power in LF\n",
    "    HFh highest power in HF\n",
    "    VLFp VLFa/TOTALa\n",
    "    LFp LFa/TOTALa\n",
    "    HFp HFa/TOTALa\n",
    "    LFn normalized area of LF LFa/(LFa+HFa)\n",
    "    HFn normalized area of HF HFa/(LFa+HFa)\n",
    "    LFHF ratio of LFa and HFa\n",
    "\n",
    "    return : [lf_area,hf_area,vlf_p,lf_p,hf_p,lf_n,lfhf]\n",
    "    \"\"\"\n",
    "    # f = np.linspace(0, fs/2, len(input_signal)//2+1)  # Frequency range (0 to Nyquist)\n",
    "    # # Apply a Hamming window to the input_signal\n",
    "    # window = signal.hamming(len(input_signal))\n",
    "    # input_signal_windowed = input_signal * window\n",
    "\n",
    "    # # Compute the FFT of the windowed input_signal\n",
    "    # fft = np.fft.rfft(input_signal_windowed)\n",
    "\n",
    "    # Compute the power spectral density (PSD)\n",
    "    input_signal=signal.detrend(input, type='constant')\n",
    "    f, psd =signal.welch(input_signal, nperseg=1024,window='hamming')\n",
    "\n",
    "    # Define the frequency bands\n",
    "    vlf_band = (0.003, 0.04)  # Very low frequency (0.003-0.04 Hz)\n",
    "    lf_band = (0.04, 0.15)    # Low frequency (0.04-0.15 Hz)\n",
    "    hf_band = (0.15, 0.4)     # High frequency (0.15-0.4 Hz)\n",
    "\n",
    "    # Compute the indices corresponding to the frequency bands\n",
    "    vlf_idx = np.where(np.logical_and(f >= vlf_band[0], f <= vlf_band[1]))[0]\n",
    "    lf_idx = np.where(np.logical_and(f >= lf_band[0], f <= lf_band[1]))[0]\n",
    "    hf_idx = np.where(np.logical_and(f >= hf_band[0], f <= hf_band[1]))[0]\n",
    "    # Compute the area under the PSD curve for each band\n",
    "    vlf_area = np.trapz(psd[vlf_idx], f[vlf_idx])\n",
    "    lf_area = np.trapz(psd[lf_idx], f[lf_idx])\n",
    "    hf_area = np.trapz(psd[hf_idx], f[hf_idx])\n",
    "    total_area = np.trapz(psd, f)\n",
    "    # Find the frequency at which the PSD has the highest value for each band\n",
    "    vlf_h = f[vlf_idx[np.argmax(psd[vlf_idx])]]\n",
    "    lf_h = f[lf_idx[np.argmax(psd[lf_idx])]]\n",
    "    hf_h = f[hf_idx[np.argmax(psd[hf_idx])]]\n",
    "\n",
    "    max_power=np.max(psd)\n",
    "\n",
    "    # Compute the VLFp, LFp, and HFp features\n",
    "    vlf_p = vlf_area / total_area\n",
    "    lf_p = lf_area / total_area\n",
    "    hf_p = hf_area / total_area\n",
    "\n",
    "    # Compute the LFn and HFn features\n",
    "    lf_n = lf_area / (lf_area + hf_area)\n",
    "    hf_n = hf_area / (lf_area + hf_area)\n",
    "\n",
    "    # Compute the LF/HF ratio (LFHF)\n",
    "    lfhf = lf_area / hf_area\n",
    "    #  VLFa, LFa,\n",
    "    # HFa, TOTALa, VLFp, LFp, HFp\n",
    "    ppg_freqs, ppg_psd = signal.welch(input, fs=128, nperseg=256)\n",
    "    alpha = np.mean(ppg_psd[(ppg_freqs >= 8) & (ppg_freqs <= 13)])\n",
    "    beta = np.mean(ppg_psd[(ppg_freqs >= 13) & (ppg_freqs <= 30)])\n",
    "    gamma = np.mean(ppg_psd[(ppg_freqs >= 30) & (ppg_freqs <= 100)])\n",
    "    theta = np.mean(ppg_psd[(ppg_freqs >= 4) & (ppg_freqs <= 8)])\n",
    "    delta = np.mean(ppg_psd[(ppg_freqs >= 0.5) & (ppg_freqs <= 4)])\n",
    "    return [vlf_area,lf_area,hf_area,total_area,vlf_p,lf_p,hf_p,vlf_h,lf_h,hf_h,lf_n,hf_n,lfhf,max_power,alpha,beta,gamma,theta,delta]\n",
    "\n",
    "def extract_features(input):\n",
    "    return freq_domain_features_from_signal(input)+nn_intervals_time_domain_features(input)+gsr_features(input)+onecycle(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=np.genfromtxt('dataset_transoform/GSR/gsr_01.csv',delimiter=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_features(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import skew,kurtosis\n",
    "from ast import literal_eval\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "from scipy import signal\n",
    "from scipy.fftpack import rfft, irfft, fftfreq, fft,ifft\n",
    "from scipy.signal import find_peaks, peak_prominences\n",
    "from numpy import trapz\n",
    "from scipy.stats import skew as find_skew\n",
    "import numpy as np\n",
    "from scipy.stats import skew\n",
    "\n",
    "def motion_removal(data):\n",
    "    \n",
    "    Fs = 100;                                                   #Sampling Frequency (Hz)\n",
    "    Fn = Fs/2;                                                  # Nyquist Frequency (Hz)\n",
    "    Ws = 0.5/Fn;                                                # Passband Frequency Vector (Normalised)\n",
    "    Wp = 1.5/Fn;                                                # Stopband Frequency Vector (Normalised)\n",
    "    Rp =   1;                                                   # Passband Ripple (dB)\n",
    "    Rs = 150; \n",
    "    N, Wn = signal.ellipord(Wp,Ws,Rp,Rs)\n",
    "    z,p,k = signal.ellip(N,Rp,Rs,Wn,'high',output='zpk')\n",
    "    sos= signal.zpk2sos(z, p, k)\n",
    "    y = signal.sosfiltfilt(sos, data)\n",
    "    return y\n",
    "def plotting(ir):\n",
    "    plt.figure(figsize=(16,4))\n",
    "    plt.plot(ir,label='IR')\n",
    "    plt.legend()\n",
    "    \n",
    "def normalize_values(read_file,write_file):\n",
    "    # Load CSV file\n",
    "    df = pd.read_csv(read_file, header=None,delimiter=',')\n",
    "\n",
    "    # # Transpose dataframe to treat first row as values\n",
    "    df = df.transpose()\n",
    "\n",
    "    # Normalize values using min-max scaling\n",
    "    df = (df - df.min()) / (df.max() - df.min())\n",
    "\n",
    "    # # Transpose dataframe back to original format\n",
    "    df = df.transpose()\n",
    "\n",
    "    # Save normalized values to CSV file\n",
    "    df.to_csv(write_file, header=None, index=None)\n",
    "\n",
    "def data_builder(input_signal,val,arr):\n",
    "    if(val>=5): val_class=1\n",
    "    else: val_class=0\n",
    "    if(arr>=5): arr_class=1\n",
    "    else: arr_class=0\n",
    "    return extract_features(input_signal)+[val,arr]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "infiles=os.listdir('dataset_transoform/GSR/')\n",
    "tarfiles=os.listdir('dataset/labels/')\n",
    "\n",
    "data_ext=np.empty((800,66))\n",
    "ind=0\n",
    "for file in range(len(infiles)):\n",
    "    data=np.genfromtxt('dataset_transoform/GSR/'+infiles[file],delimiter=',')\n",
    "    target=np.genfromtxt('dataset/labels/'+tarfiles[file],delimiter=',')\n",
    "    print(data.shape)\n",
    "    print(target.shape)\n",
    "    for i in range(len(data)):\n",
    "        stats_feature=data_builder(data[i],target[i][0],target[i][1])\n",
    "        data_ext[ind]=np.array(stats_feature)\n",
    "        ind+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df=pd.DataFrame(data_ext,index=None,columns=[\"vlf_area\",\"lf_area\",\"hf_area\",\"total_area\",\"vlf_p\",\"lf_p\",\"hf_p\",\"vlf_h\",\"lf_h\",\"hf_h\",\"lf_n\",\"hf_n\",\"lfhf\",\"max_power\",\"alpha\",\"beta\",\"gamma\",\"theta\",\"delta\",\"meanNN\",\"medianNN\",\"nn50\",\"pNN50\",\"rmssd\",\"sdnn\",\"scl\",\"scr\",\"rise_time\",\"half_recovery_time\",\"peak_amp\",\"auc\",\"n_scr\",\"slope\",\"intercept\",\"mean_latency\",\"mean_duration\",\"hrv_rmssd\",\"hrv_sdnn\",\" hrv_mean\",\"mean\",\"median\",\"std\",\"kurt\",\"skewness\",\"p10\",\"p25\",\"p30\",\"p50\",\"p75\",\"p80\",\"p90\",\"IQR\",\"EN\",\"mid_AD\",\"mean_AD\",\"RMS\",\"spec_EN\",\"activity\",\"mobility\",\"complexity\",\"env_mean\",\"env_std\",\"am_mean\",\"am_std\",\"val\",\"arr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('gsr_features_64.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [00:00<00:00, 108.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Accuracy, Balanced Accuracy, ROC AUC, F1 Score, Time Taken]\n",
      "Index: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## importing lazypredict library\n",
    "import lazypredict\n",
    "### importing LazyClassifier for classification problem\n",
    "from lazypredict.Supervised import LazyClassifier\n",
    "### importing LazyClassifier for classification problem because here we are solving Classification use case.\n",
    "from lazypredict.Supervised import LazyClassifier\n",
    "### importing breast Cancer Dataset from sklearn\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "### spliting dataset into training and testing part\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.iloc[:,:-2].to_numpy(),df.iloc[:,-1:].to_numpy().reshape(800), test_size=.5, random_state =123)\n",
    "clf = LazyClassifier(verbose=0, ignore_warnings=True, custom_metric = None)\n",
    "### fitting data in LazyClassifier\n",
    "models,predictions = clf.fit(X_train, X_test, y_train, y_test)\n",
    "### lets check which model did better on Breast Cancer Dataset\n",
    "print(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.24169325e-03,  2.91328920e-04,  1.57334995e-03,  3.42971469e-03,\n",
       "        3.62039809e-01,  8.49426108e-02,  4.58740766e-01,  1.85546875e-02,\n",
       "        1.22070312e-01,  2.49023438e-01,  1.56235439e-01,  8.43764561e-01,\n",
       "        1.85164732e-01,  2.61167683e-01,  1.56184382e-05,  8.49751572e-06,\n",
       "        5.67859791e-05,  5.07894209e-05,  2.89817429e-04,  5.02835148e-01,\n",
       "        5.02603862e-01,  1.45700000e+03,  1.80679563e+01,  6.83624946e-02,\n",
       "        5.90566264e-02,  5.02835148e-01,  1.00000000e+00,  1.30000000e+01,\n",
       "        1.16000000e+02,  5.72942942e-01,  6.04435564e+01,  5.78000000e+02,\n",
       "       -2.15693163e-08,  5.02922105e-01,  4.53119723e+03,  1.36759099e+01,\n",
       "        7.92847373e+01,  5.48350028e+01,  6.88620690e+01,  5.02835148e-01,\n",
       "        5.02603862e-01,  5.90566264e-02,  6.71997836e+00, -1.10255040e-01,\n",
       "        4.42053718e-01,  4.74154783e-01,  4.80393732e-01,  5.02603862e-01,\n",
       "        5.32281166e-01,  5.39856897e-01,  5.63330842e-01,  5.81263828e-02,\n",
       "        8.98802709e+00,  2.90344609e-02,  4.06761703e-02,  5.06291291e-01,\n",
       "        6.92597150e-01,  3.48768512e-03,  1.15757534e+00,  1.59440196e+00,\n",
       "        5.06408582e-01,  5.80421193e-02,  9.92558330e-01,  2.63366675e-02,\n",
       "        7.71000000e+00,  7.60000000e+00])"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.to_numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
