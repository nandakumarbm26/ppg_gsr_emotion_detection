{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef096de2-66bf-49b6-a477-7c3de673a0a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "def normalize_values(read_file,write_file):\n",
    "    # Load CSV file\n",
    "    df = pd.read_csv(read_file, header=None,delimiter=',')\n",
    "\n",
    "    # # Transpose dataframe to treat first row as values\n",
    "    df = df.transpose()\n",
    "\n",
    "    # Normalize values using min-max scaling\n",
    "    df = (df - df.min()) / (df.max() - df.min())\n",
    "\n",
    "    # # Transpose dataframe back to original format\n",
    "    df = df.transpose()\n",
    "\n",
    "    # Save normalized values to CSV file\n",
    "    df.to_csv(write_file, header=None, index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f59af0-30dc-4915-84e3-5e4e51e9e711",
   "metadata": {},
   "source": [
    "MOTION REMOVAL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b742f76-09d7-4928-a9f1-48405e184c63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import skew,kurtosis\n",
    "from ast import literal_eval\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "from scipy import signal\n",
    "from scipy.fftpack import rfft, irfft, fftfreq, fft,ifft\n",
    "from scipy.signal import find_peaks, peak_prominences\n",
    "from numpy import trapz\n",
    "from scipy.stats import skew as find_skew\n",
    "import numpy as np\n",
    "from scipy.stats import skew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fc8be3-6052-46d5-b0f8-615b7346767d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def motion_removal(data):\n",
    "    \n",
    "    Fs = 100;                                                   #Sampling Frequency (Hz)\n",
    "    Fn = Fs/2;                                                  # Nyquist Frequency (Hz)\n",
    "    Ws = 0.5/Fn;                                                # Passband Frequency Vector (Normalised)\n",
    "    Wp = 1.5/Fn;                                                # Stopband Frequency Vector (Normalised)\n",
    "    Rp =   1;                                                   # Passband Ripple (dB)\n",
    "    Rs = 150; \n",
    "    N, Wn = signal.ellipord(Wp,Ws,Rp,Rs)\n",
    "    z,p,k = signal.ellip(N,Rp,Rs,Wn,'high',output='zpk')\n",
    "    sos= signal.zpk2sos(z, p, k)\n",
    "    y = signal.sosfiltfilt(sos, data)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985f2a49-96ea-42b3-af1a-277ec21a9655",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plotting(ir):\n",
    "    plt.figure(figsize=(16,4))\n",
    "    plt.plot(ir,label='ppg')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299440fe-0e43-4e95-bbf3-9b9624001cdf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "folder_path = '.\\ppg_new'\n",
    "# Loop over all the files in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    # Check if the file is a CSV file\n",
    "    if filename.endswith('.csv'):\n",
    "        # Do something with the file\n",
    "        df=pd.read_csv(filename,header=None)\n",
    "        motion_removal(df)\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        print(f'Processing file: {file_path}')\n",
    "        # Perform your desired tasks on the file here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "347a1f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.layers import Input,Dropout, Dense, Conv1D, BatchNormalization, MaxPooling1D, concatenate, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# define input layers for CNN1\n",
    "input1 = Input(shape=(240, 1))\n",
    "\n",
    "# CNN1\n",
    "conv1 = Conv1D(20, 3, padding='same', activation='relu')(input1)\n",
    "bn1 = BatchNormalization()(conv1)\n",
    "pool1 = MaxPooling1D(2)(bn1)\n",
    "\n",
    "conv2 = Conv1D(20, 3, padding='same', activation='relu')(pool1)\n",
    "bn2 = BatchNormalization()(conv2)\n",
    "pool2 = MaxPooling1D(2)(bn2)\n",
    "\n",
    "conv3 = Conv1D(20, 3, padding='same', activation='relu')(pool2)\n",
    "bn3 = BatchNormalization()(conv3)\n",
    "pool3 = MaxPooling1D(2)(bn3)\n",
    "\n",
    "# flatten CNN1 output\n",
    "flatten1 = Flatten()(pool3)\n",
    "\n",
    "# define input layers for CNN2\n",
    "input2 = Input(shape=(9, 1))\n",
    "\n",
    "# CNN2\n",
    "conv1_nn = Conv1D(20, 3, padding='same', activation='relu')(input2)\n",
    "bn1_nn = BatchNormalization()(conv1_nn)\n",
    "pool1_nn = MaxPooling1D(2)(bn1_nn)\n",
    "\n",
    "conv2_nn = Conv1D(20, 3, padding='same', activation='relu')(pool1_nn)\n",
    "bn2_nn = BatchNormalization()(conv2_nn)\n",
    "pool2_nn = MaxPooling1D(2)(bn2_nn)\n",
    "\n",
    "conv3_nn = Conv1D(20, 3, padding='same', activation='relu')(pool2_nn)\n",
    "bn3_nn = BatchNormalization()(conv3_nn)\n",
    "pool3_nn = MaxPooling1D(2)(bn3_nn)\n",
    "\n",
    "# flatten CNN2 output\n",
    "flatten2 = Flatten()(pool3_nn)\n",
    "\n",
    "# define input layer for statistical features\n",
    "input3 = Input(shape=(10,))\n",
    "\n",
    "# concatenate all features\n",
    "concatenated = concatenate([flatten1, flatten2, input3])\n",
    "flat1=Flatten()(concatenated)\n",
    "# add Dense layers for classification\n",
    "dense1 = Dense(500, activation='relu')(flat1)\n",
    "dropout1 = Dropout(0.5)(dense1)\n",
    "dense2 = Dense(2, activation='softmax')(dropout1)\n",
    "\n",
    "# define model\n",
    "model = Model(inputs=[input1, input2, input3], outputs=dense2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "e6df6892",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "  optimizer='adam',\n",
    "  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "84a6a015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_22\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_72 (InputLayer)          [(None, 240, 1)]     0           []                               \n",
      "                                                                                                  \n",
      " input_73 (InputLayer)          [(None, 9, 1)]       0           []                               \n",
      "                                                                                                  \n",
      " conv1d_118 (Conv1D)            (None, 240, 20)      80          ['input_72[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_121 (Conv1D)            (None, 9, 20)        80          ['input_73[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_74 (BatchN  (None, 240, 20)     80          ['conv1d_118[0][0]']             \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_77 (BatchN  (None, 9, 20)       80          ['conv1d_121[0][0]']             \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " max_pooling1d_70 (MaxPooling1D  (None, 120, 20)     0           ['batch_normalization_74[0][0]'] \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " max_pooling1d_73 (MaxPooling1D  (None, 4, 20)       0           ['batch_normalization_77[0][0]'] \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv1d_119 (Conv1D)            (None, 120, 20)      1220        ['max_pooling1d_70[0][0]']       \n",
      "                                                                                                  \n",
      " conv1d_122 (Conv1D)            (None, 4, 20)        1220        ['max_pooling1d_73[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_75 (BatchN  (None, 120, 20)     80          ['conv1d_119[0][0]']             \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_78 (BatchN  (None, 4, 20)       80          ['conv1d_122[0][0]']             \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " max_pooling1d_71 (MaxPooling1D  (None, 60, 20)      0           ['batch_normalization_75[0][0]'] \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " max_pooling1d_74 (MaxPooling1D  (None, 2, 20)       0           ['batch_normalization_78[0][0]'] \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv1d_120 (Conv1D)            (None, 60, 20)       1220        ['max_pooling1d_71[0][0]']       \n",
      "                                                                                                  \n",
      " conv1d_123 (Conv1D)            (None, 2, 20)        1220        ['max_pooling1d_74[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_76 (BatchN  (None, 60, 20)      80          ['conv1d_120[0][0]']             \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_79 (BatchN  (None, 2, 20)       80          ['conv1d_123[0][0]']             \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " max_pooling1d_72 (MaxPooling1D  (None, 30, 20)      0           ['batch_normalization_76[0][0]'] \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " max_pooling1d_75 (MaxPooling1D  (None, 1, 20)       0           ['batch_normalization_79[0][0]'] \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " flatten_58 (Flatten)           (None, 600)          0           ['max_pooling1d_72[0][0]']       \n",
      "                                                                                                  \n",
      " flatten_59 (Flatten)           (None, 20)           0           ['max_pooling1d_75[0][0]']       \n",
      "                                                                                                  \n",
      " input_74 (InputLayer)          [(None, 10)]         0           []                               \n",
      "                                                                                                  \n",
      " concatenate_35 (Concatenate)   (None, 630)          0           ['flatten_58[0][0]',             \n",
      "                                                                  'flatten_59[0][0]',             \n",
      "                                                                  'input_74[0][0]']               \n",
      "                                                                                                  \n",
      " flatten_60 (Flatten)           (None, 630)          0           ['concatenate_35[0][0]']         \n",
      "                                                                                                  \n",
      " dense_44 (Dense)               (None, 500)          315500      ['flatten_60[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_9 (Dropout)            (None, 500)          0           ['dense_44[0][0]']               \n",
      "                                                                                                  \n",
      " dense_45 (Dense)               (None, 2)            1002        ['dropout_9[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 322,022\n",
      "Trainable params: 321,782\n",
      "Non-trainable params: 240\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e5cdb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.signal import find_peaks\n",
    "import matplotlib.pyplot as plt\n",
    "df=pd.read_csv('./dataset_transoform/PPG/ppg_01.csv',header=None)\n",
    "\n",
    "def nn_intervals_from_signal(signal,sampling_freq:int):\n",
    "    dt = 1/sampling_freq # Sampling interval\n",
    "    peaks, _ = find_peaks(signal, height=0.5*np.max(signal),distance=100)\n",
    "    peaks_diff=peaks*dt\n",
    "    nn_intervals = np.diff(peaks_diff)\n",
    "    return nn_intervals\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa98f8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_intervals_time_domain_features(input_signal,fs):\n",
    "    nn_intervals=nn_intervals_from_signal(input_signal,fs)\n",
    "    nni=np.zeros((9,1))\n",
    "    \n",
    "    for i in range(len(nn_intervals)):\n",
    "        if(i<9):\n",
    "            nni[i][0]=nn_intervals[i]\n",
    "    meanNN = np.mean(nn_intervals)\n",
    "    medianNN = np.median(nn_intervals)\n",
    "    sdnn = np.std(nn_intervals)\n",
    "    nn50 = [x>0.05 for x in np.diff(nn_intervals)].count(True)\n",
    "    pNN50 = nn50 / len(nn_intervals) * 100\n",
    "    rmssd = np.sqrt(np.mean(np.square(np.diff(nn_intervals))))\n",
    "    return nni,[meanNN,medianNN,nn50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a65c78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import signal as sig\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "def freq_domain_features_from_signal(input_signal):\n",
    "    \"\"\" \n",
    "    params : input_signal -> np array\n",
    "    params: fs -> int\n",
    "\n",
    "    VLFa area of VLF (0-0.04hz)\n",
    "    LFa area of VLF   (0.0.4-0.15hz)\n",
    "    HFa area of VLF   (0.15-0.4hz)\n",
    "    TOTALa total area of all band\n",
    "    VLFh highest power in VLF\n",
    "    LFh highest power in LF\n",
    "    HFh highest power in HF\n",
    "    VLFp VLFa/TOTALa\n",
    "    LFp LFa/TOTALa\n",
    "    HFp HFa/TOTALa\n",
    "    LFn normalized area of LF LFa/(LFa+HFa)\n",
    "    HFn normalized area of HF HFa/(LFa+HFa)\n",
    "    LFHF ratio of LFa and HFa\n",
    "\n",
    "    return : [lf_area,hf_area,vlf_p,lf_p,hf_p,lf_n,lfhf]\n",
    "    \"\"\"\n",
    "    # f = np.linspace(0, fs/2, len(input_signal)//2+1)  # Frequency range (0 to Nyquist)\n",
    "    # # Apply a Hamming window to the input_signal\n",
    "    # window = signal.hamming(len(input_signal))\n",
    "    # input_signal_windowed = input_signal * window\n",
    "\n",
    "    # # Compute the FFT of the windowed input_signal\n",
    "    # fft = np.fft.rfft(input_signal_windowed)\n",
    "\n",
    "    # Compute the power spectral density (PSD)\n",
    "    input_signal=sig.detrend(input_signal, type='constant')\n",
    "    f, psd =sig.welch(input_signal, nperseg=1024,window='hamming')\n",
    "\n",
    "    # Define the frequency bands\n",
    "    vlf_band = (0.003, 0.04)  # Very low frequency (0.003-0.04 Hz)\n",
    "    lf_band = (0.04, 0.15)    # Low frequency (0.04-0.15 Hz)\n",
    "    hf_band = (0.15, 0.4)     # High frequency (0.15-0.4 Hz)\n",
    "\n",
    "    # Compute the indices corresponding to the frequency bands\n",
    "    vlf_idx = np.where(np.logical_and(f >= vlf_band[0], f <= vlf_band[1]))[0]\n",
    "    lf_idx = np.where(np.logical_and(f >= lf_band[0], f <= lf_band[1]))[0]\n",
    "    hf_idx = np.where(np.logical_and(f >= hf_band[0], f <= hf_band[1]))[0]\n",
    "    # Compute the area under the PSD curve for each band\n",
    "    vlf_area = np.trapz(psd[vlf_idx], f[vlf_idx])\n",
    "    lf_area = np.trapz(psd[lf_idx], f[lf_idx])\n",
    "    hf_area = np.trapz(psd[hf_idx], f[hf_idx])\n",
    "    total_area = np.trapz(psd, f)\n",
    "    # Find the frequency at which the PSD has the highest value for each band\n",
    "    # vlf_h = f[vlf_idx[np.argmax(psd[vlf_idx])]]\n",
    "    # lf_h = f[lf_idx[np.argmax(psd[lf_idx])]]\n",
    "    # hf_h = f[hf_idx[np.argmax(psd[hf_idx])]]\n",
    "\n",
    "    # Compute the VLFp, LFp, and HFp features\n",
    "    vlf_p = vlf_area / total_area\n",
    "    lf_p = lf_area / total_area\n",
    "    hf_p = hf_area / total_area\n",
    "\n",
    "    # Compute the LFn and HFn features\n",
    "    lf_n = lf_area / (lf_area + hf_area)\n",
    "    hf_n = hf_area / (lf_area + hf_area)\n",
    "\n",
    "    # Compute the LF/HF ratio (LFHF)\n",
    "    lfhf = lf_area / hf_area\n",
    "    #  VLFa, LFa,\n",
    "    # HFa, TOTALa, VLFp, LFp, HFp\n",
    "    return [vlf_area,lf_area,hf_area,total_area,vlf_p,lf_p,hf_p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84daa564",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_builder(input_signal,val,arr):\n",
    "    nn_intervals,time_domain_features=nn_intervals_time_domain_features(input_signal,128)\n",
    "    freq_domain_features=freq_domain_features_from_signal(input_signal)\n",
    "    time_domain_features=time_domain_features+freq_domain_features\n",
    "    if(val>=5): val_class=[0,1]\n",
    "    else: val_class=[1,0]\n",
    "    if(arr>=5): arr_class=[0,1]\n",
    "    else: arr_class=[1,0]\n",
    "    return input_signal[:240],nn_intervals,time_domain_features,val_class,arr_class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf9043f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "infiles=os.listdir('dataset_transoform/GSR/')\n",
    "tarfiles=os.listdir('dataset/labels/')\n",
    "\n",
    "signals=np.empty((4800,240,1))\n",
    "nnis=np.empty((4800,9,1))\n",
    "stats_features=np.empty((4800,10,1))\n",
    "valclasses=np.empty((4800,2))\n",
    "arrclasses=np.empty((4800,2))\n",
    "ind=0\n",
    "for file in range(len(infiles)):\n",
    "    data=np.genfromtxt('dataset_transoform/GSR/'+infiles[file],delimiter=',')\n",
    "    target=np.genfromtxt('dataset/labels/'+tarfiles[file],delimiter=',')\n",
    "    print(data.shape)\n",
    "    print(target.shape)\n",
    "    for i in range(len(data)):\n",
    "        for f in range(6):\n",
    "            signal,nn_intervals,stats_feature,valclass,arrclass=data_builder(data[i][384+1280*f:384+1280*(f+1)],target[i][0],target[i][1])\n",
    "            signals[ind]=signal.reshape(240,1)\n",
    "            nnis[ind]=nn_intervals\n",
    "            # print(nn_intervals)\n",
    "            stats_features[ind]=np.array(stats_feature).reshape(10,1)\n",
    "            valclasses[ind]=valclass\n",
    "            arrclasses[ind]=arrclass\n",
    "            ind+=1\n",
    "signals=np.array(signals)\n",
    "nnis=np.array(nnis)\n",
    "stats_features=np.array(stats_features)\n",
    "valclasses=np.array(valclasses)\n",
    "arrclasses=np.array(valclasses)\n",
    "print(signals.shape)\n",
    "print(nnis.shape)\n",
    "print(stats_features.shape)\n",
    "print(valclasses.shape)\n",
    "print(arrclasses.shape)\n",
    "input=[signals,nnis,stats_features]\n",
    "# np.array(input).tofile('dat.csv',sep=',')\n",
    "label_val=valclasses\n",
    "label_arr=arrclasses\n",
    "# np.array(label_arr).tofile('labv.csv',sep=',')\n",
    "# np.array(label_val).tofile('laba.csv',sep=',')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "fa42eb0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "120/120 [==============================] - 6s 22ms/step - loss: 1.1547 - accuracy: 0.5029 - val_loss: 0.6836 - val_accuracy: 0.5844\n",
      "Epoch 2/10\n",
      "120/120 [==============================] - 2s 20ms/step - loss: 0.7219 - accuracy: 0.5690 - val_loss: 0.7009 - val_accuracy: 0.5156\n",
      "Epoch 3/10\n",
      "120/120 [==============================] - 2s 20ms/step - loss: 0.6566 - accuracy: 0.6172 - val_loss: 0.7236 - val_accuracy: 0.5073\n",
      "Epoch 4/10\n",
      "120/120 [==============================] - 2s 19ms/step - loss: 0.5904 - accuracy: 0.6812 - val_loss: 0.7320 - val_accuracy: 0.5146\n",
      "Epoch 5/10\n",
      "120/120 [==============================] - 2s 18ms/step - loss: 0.5681 - accuracy: 0.6919 - val_loss: 0.7586 - val_accuracy: 0.4865\n",
      "Epoch 6/10\n",
      "120/120 [==============================] - 2s 19ms/step - loss: 0.5270 - accuracy: 0.7232 - val_loss: 0.7998 - val_accuracy: 0.5177\n",
      "Epoch 7/10\n",
      "120/120 [==============================] - 2s 21ms/step - loss: 0.4979 - accuracy: 0.7378 - val_loss: 0.8030 - val_accuracy: 0.5677\n",
      "Epoch 8/10\n",
      "120/120 [==============================] - 3s 22ms/step - loss: 0.4665 - accuracy: 0.7607 - val_loss: 0.8439 - val_accuracy: 0.5500\n",
      "Epoch 9/10\n",
      "120/120 [==============================] - 2s 19ms/step - loss: 0.4425 - accuracy: 0.7802 - val_loss: 0.8728 - val_accuracy: 0.4604\n",
      "Epoch 10/10\n",
      "120/120 [==============================] - 2s 19ms/step - loss: 0.4132 - accuracy: 0.8010 - val_loss: 0.8326 - val_accuracy: 0.5323\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22307707110>"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(input, label_val, epochs=10, batch_size=32,  validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2df8242",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Generate example input data\n",
    "ppg_data = np.random.randn(1000, 240, 1)\n",
    "ecg_data = np.random.randn(1000, 9, 1)\n",
    "stats_data = np.random.randn(1000, 10)\n",
    "\n",
    "# Combine input data into a tuple\n",
    "input_data = (ppg_data, ecg_data, stats_data)\n",
    "# Generate example binary labels\n",
    "binary_labels = np.random.randint(2, size=(1000,))\n",
    "\n",
    "# Convert binary labels to one-hot encoding\n",
    "one_hot_labels = np.zeros((1000, 2))\n",
    "one_hot_labels[np.arange(1000), binary_labels] = 1\n",
    "\n",
    "# Assign targets variable\n",
    "targets = one_hot_labels\n",
    "# Train the model\n",
    "# model.fit(input_data, targets, epochs=10, batch_size=32, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee211be",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69100e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict([signals[4790].reshape(1,240,1),nnis[4790].reshape(1,9,1),stats_features[4790].reshape(1,10,1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a530298f",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_val[4790]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e5d711",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('cnn_100ep_arousal.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0c1a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv1D, BatchNormalization, MaxPooling1D, Flatten, Dense, Dropout, Concatenate, Activation\n",
    "\n",
    "# Input layers\n",
    "ppg_input = Input(shape=(240, 1), name='ppg_input')\n",
    "nn_input = Input(shape=(9, 1), name='nn_input')\n",
    "stat_input = Input(shape=(10,), name='stat_input')\n",
    "\n",
    "# First CNN layers\n",
    "conv1 = Conv1D(20, kernel_size=3, padding='same')(ppg_input)\n",
    "bn1 = BatchNormalization()(conv1)\n",
    "relu1 = Activation('relu')(bn1)\n",
    "pool1 = MaxPooling1D(pool_size=2)(relu1)\n",
    "conv2 = Conv1D(20, kernel_size=3, padding='same')(pool1)\n",
    "bn2 = BatchNormalization()(conv2)\n",
    "relu2 = Activation('relu')(bn2)\n",
    "pool2 = MaxPooling1D(pool_size=2)(relu2)\n",
    "conv3 = Conv1D(20, kernel_size=3, padding='same')(pool2)\n",
    "bn3 = BatchNormalization()(conv3)\n",
    "relu3 = Activation('relu')(bn3)\n",
    "pool3 = MaxPooling1D(pool_size=2)(relu3)\n",
    "\n",
    "# Second CNN layers\n",
    "conv1_nn = Conv1D(10, kernel_size=3, padding='same')(nn_input)\n",
    "bn1_nn = BatchNormalization()(conv1_nn)\n",
    "relu1_nn = Activation('relu')(bn1_nn)\n",
    "pool1_nn = MaxPooling1D(pool_size=2)(relu1_nn)\n",
    "conv2_nn = Conv1D(10, kernel_size=3, padding='same')(pool1_nn)\n",
    "bn2_nn = BatchNormalization()(conv2_nn)\n",
    "relu2_nn = Activation('relu')(bn2_nn)\n",
    "pool2_nn = MaxPooling1D(pool_size=2)(relu2_nn)\n",
    "\n",
    "# Concatenate all features\n",
    "flatten = Concatenate()([\n",
    "    Flatten()(pool3),\n",
    "    Flatten()(pool2_nn),\n",
    "    stat_input\n",
    "])\n",
    "\n",
    "# Dense layers\n",
    "dense1 = Dense(200, activation='relu')(flatten)\n",
    "dropout1 = Dropout(0.5)(dense1)\n",
    "dense2 = Dense(500, activation='relu')(dropout1)\n",
    "dropout2 = Dropout(0.5)(dense2)\n",
    "output = Dense(2, activation='softmax')(dropout2)\n",
    "\n",
    "# Define model\n",
    "model = Model(inputs=[ppg_input, nn_input, stat_input], outputs=output)\n",
    "\n",
    "# Compile model with L2 regularization\n",
    "from tensorflow.keras import regularizers\n",
    "reg = regularizers.l2(0.001)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcb5b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "r=model.fit(input,label_val,epochs=20,batch_size=16,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a0feb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(r.history['loss'], label='train loss')\n",
    "plt.plot(r.history['val_loss'], label='val loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.savefig('LossVal_loss')\n",
    "\n",
    "# plot the accuracy\n",
    "plt.plot(r.history['accuracy'], label='train acc')\n",
    "plt.plot(r.history['val_accuracy'], label='val acc')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.savefig('AccVal_acc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08277c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Conv1D, Flatten, concatenate, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "# Define input layers\n",
    "input1 = Input(shape=(240, 1))\n",
    "input2 = Input(shape=(9, 1))\n",
    "\n",
    "# First CNN layer for input1\n",
    "conv1_1 = Conv1D(filters=32, kernel_size=3, activation='relu')(input1)\n",
    "# Second CNN layer for input1\n",
    "conv1_2 = Conv1D(filters=64, kernel_size=3, activation='relu')(conv1_1)\n",
    "# Flatten output from input1\n",
    "flat1 = Flatten()(conv1_2)\n",
    "\n",
    "# First CNN layer for input2\n",
    "conv2_1 = Conv1D(filters=16, kernel_size=3, activation='relu')(input2)\n",
    "# Second CNN layer for input2\n",
    "conv2_2 = Conv1D(filters=32, kernel_size=3, activation='relu')(conv2_1)\n",
    "# Flatten output from input2\n",
    "flat2 = Flatten()(conv2_2)\n",
    "\n",
    "# Concatenate feature maps from both inputs\n",
    "merged = concatenate([flat1, flat2], axis=1)\n",
    "\n",
    "# Define input for statistical features\n",
    "input3 = Input(shape=(10,))\n",
    "\n",
    "# Concatenate statistical features with feature vector\n",
    "merged2 = concatenate([merged, input3], axis=1)\n",
    "\n",
    "# Fully connected layer for classification\n",
    "output = Dense(2, activation='softmax')(merged2)\n",
    "\n",
    "# Define the model\n",
    "model = Model(inputs=[input1, input2, input3], outputs=output)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "ac6eb3a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [00:06<00:00,  4.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               Accuracy  Balanced Accuracy  ROC AUC  F1 Score   \n",
      "Model                                                                           \n",
      "RandomForestClassifier             0.56               0.55     0.55      0.56  \\\n",
      "XGBClassifier                      0.56               0.55     0.55      0.55   \n",
      "ExtraTreesClassifier               0.56               0.55     0.55      0.55   \n",
      "LGBMClassifier                     0.55               0.54     0.54      0.54   \n",
      "BaggingClassifier                  0.55               0.54     0.54      0.54   \n",
      "QuadraticDiscriminantAnalysis      0.54               0.53     0.53      0.54   \n",
      "AdaBoostClassifier                 0.55               0.53     0.53      0.53   \n",
      "NearestCentroid                    0.54               0.53     0.53      0.53   \n",
      "ExtraTreeClassifier                0.53               0.53     0.53      0.53   \n",
      "LinearDiscriminantAnalysis         0.56               0.53     0.53      0.50   \n",
      "RidgeClassifierCV                  0.56               0.53     0.53      0.50   \n",
      "RidgeClassifier                    0.56               0.53     0.53      0.50   \n",
      "NuSVC                              0.53               0.53     0.53      0.53   \n",
      "LinearSVC                          0.56               0.53     0.53      0.50   \n",
      "LogisticRegression                 0.56               0.53     0.53      0.50   \n",
      "CalibratedClassifierCV             0.56               0.52     0.52      0.49   \n",
      "GaussianNB                         0.55               0.52     0.52      0.51   \n",
      "Perceptron                         0.55               0.52     0.52      0.51   \n",
      "SVC                                0.55               0.52     0.52      0.49   \n",
      "LabelPropagation                   0.52               0.52     0.52      0.52   \n",
      "PassiveAggressiveClassifier        0.52               0.52     0.52      0.52   \n",
      "DecisionTreeClassifier             0.52               0.51     0.51      0.52   \n",
      "LabelSpreading                     0.51               0.51     0.51      0.51   \n",
      "KNeighborsClassifier               0.52               0.51     0.51      0.51   \n",
      "BernoulliNB                        0.53               0.51     0.51      0.50   \n",
      "DummyClassifier                    0.55               0.50     0.50      0.39   \n",
      "SGDClassifier                      0.52               0.50     0.50      0.49   \n",
      "\n",
      "                               Time Taken  \n",
      "Model                                      \n",
      "RandomForestClassifier               1.05  \n",
      "XGBClassifier                        0.20  \n",
      "ExtraTreesClassifier                 0.42  \n",
      "LGBMClassifier                       0.10  \n",
      "BaggingClassifier                    0.30  \n",
      "QuadraticDiscriminantAnalysis        0.02  \n",
      "AdaBoostClassifier                   0.32  \n",
      "NearestCentroid                      0.02  \n",
      "ExtraTreeClassifier                  0.03  \n",
      "LinearDiscriminantAnalysis           0.02  \n",
      "RidgeClassifierCV                    0.03  \n",
      "RidgeClassifier                      0.02  \n",
      "NuSVC                                1.73  \n",
      "LinearSVC                            0.12  \n",
      "LogisticRegression                   0.05  \n",
      "CalibratedClassifierCV               0.41  \n",
      "GaussianNB                           0.02  \n",
      "Perceptron                           0.02  \n",
      "SVC                                  0.74  \n",
      "LabelPropagation                     0.39  \n",
      "PassiveAggressiveClassifier          0.02  \n",
      "DecisionTreeClassifier               0.05  \n",
      "LabelSpreading                       0.55  \n",
      "KNeighborsClassifier                 0.13  \n",
      "BernoulliNB                          0.02  \n",
      "DummyClassifier                      0.01  \n",
      "SGDClassifier                        0.02  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## importing lazypredict library\n",
    "import lazypredict\n",
    "### importing LazyClassifier for classification problem\n",
    "from lazypredict.Supervised import LazyClassifier\n",
    "### importing LazyClassifier for classification problem because here we are solving Classification use case.\n",
    "from lazypredict.Supervised import LazyClassifier\n",
    "### importing breast Cancer Dataset from sklearn\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "### spliting dataset into training and testing part\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(stats_features.reshape(4800,10),vl , test_size=.5, random_state =123)\n",
    "clf = LazyClassifier(verbose=0, ignore_warnings=True, custom_metric = None)\n",
    "### fitting data in LazyClassifier\n",
    "models,predictions = clf.fit(X_train, X_test, y_train, y_test)\n",
    "### lets check which model did better on Breast Cancer Dataset\n",
    "print(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f2041d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e1c05b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9cea1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "vl=np.empty((4800))\n",
    "for i in range(4800):\n",
    "    vl[i]=valclasses[i][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd522b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
